# -*- coding: utf-8 -*-
"""Submission Dissertation FinalFHS+PBTXL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tuZ_UxR-gEN-kfUq-VfhPX0mkTfVg5_k

**Student Name:** Pradip Thapa<br/>
**Student Id:** 230195578<br/>

# **Importing Libraries**
"""

# Data handling
import pandas as pd
import numpy as np
# Modeling
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report)
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import roc_curve, auc
# Preprocessing
from sklearn.preprocessing import StandardScaler
# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
# Explainability
import shap
import random
import torch

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
# using PyTorch (Pipeline B.1)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

"""# **Pipeline A: ML on Structured Clinical Data (FHS Dataset)**

# **DataSet Exploration**
"""

# Load Framingham dataset
df_fhs = pd.read_csv('framingham.csv')

# Preview data
df_fhs.head()

# Check column types
df_fhs.info()

#Descriptive Statistics
df_fhs.describe()

# Check for missing values
df_fhs.isnull().sum()

#Handle Missing Values
# Fill missing values with median (robust method)
df_fhs.fillna(df_fhs.median(), inplace=True)

# Check for missing values after handling
df_fhs.isnull().sum()

#Define Features and Target
# Define target column
target = 'TenYearCHD'

# Define feature matrix (X) and target vector (y)
X_fhs = df_fhs.drop(columns=[target])
y_fhs = df_fhs[target]

#Feature Scaling
scaler_fhs = StandardScaler()
X_fhs_scaled = scaler_fhs.fit_transform(X_fhs)

"""# **Data Visualization**"""

#Plot Target Variable Distribution
# Plot class balance
sns.countplot(data=df_fhs, x='TenYearCHD', palette='Set2')
plt.title('Distribution of Heart Disease Risk (TenYearCHD)')
plt.xlabel('Heart Disease within 10 Years (0 = No, 1 = Yes)')
plt.ylabel('Number of Patients')
plt.show()

# Also print the counts
print(df_fhs['TenYearCHD'].value_counts())

#Plot Distributions of Numerical Features
# Select numeric features
numeric_features = ['age', 'cigsPerDay', 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']
# Plot histograms
df_fhs[numeric_features].hist(bins=20, figsize=(16, 12), color='steelblue', edgecolor='black')
plt.suptitle('Distributions of Key Numeric Features')
plt.tight_layout()
plt.show()

#Correlation Heatmap
# Compute correlation matrix
corr_matrix = df_fhs.corr()

# Plot heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", square=True)
plt.title('Feature Correlation Heatmap')
plt.show()

#Boxplots – Key Features vs. Target
plt.figure(figsize=(12, 6))
sns.boxplot(data=df_fhs, x='TenYearCHD', y='age', palette='Set3')
plt.title('Age Distribution by Heart Disease Risk')
plt.show()

plt.figure(figsize=(12, 6))
sns.boxplot(data=df_fhs, x='TenYearCHD', y='sysBP', palette='Set3')
plt.title('Systolic Blood Pressure by Heart Disease Risk')
plt.show()

#Pairplot
# Sample features to avoid overcrowding
sampled_features = ['age', 'totChol', 'sysBP', 'BMI', 'TenYearCHD']

sns.pairplot(df_fhs[sampled_features], hue='TenYearCHD', palette='Set1')
plt.suptitle('Pairplot of Key Features vs Heart Disease Risk', y=1.02)
plt.show()

#Barplots for Categorical Variables
categorical_vars = ['male', 'currentSmoker', 'diabetes']

for col in categorical_vars:
    sns.countplot(data=df_fhs, x=col, hue='TenYearCHD', palette='Set2')
    plt.title(f'{col} vs TenYearCHD')
    plt.xlabel(col)
    plt.ylabel('Count')
    plt.legend(title='Heart Disease Risk')
    plt.show()

# Age Distribution by Outcome
sns.histplot(data=df_fhs, x='age', hue='TenYearCHD', kde=True, bins=30)
plt.title("Age Distribution by Heart Disease Outcome")
plt.xlabel("Age")
plt.ylabel("Frequency")
plt.show()

"""# **Model Selection and Training**"""

Xf_train, Xf_test, yf_train, yf_test = train_test_split(
    X_fhs_scaled, y_fhs, test_size=0.2, random_state=SEED, stratify=y_fhs
)

print(f"Train shape: {Xf_train.shape}, Test shape: {Xf_test.shape}")

# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

def evaluate_model(y_true, y_pred, y_proba=None):
    print(f"Accuracy:  {accuracy_score(y_true, y_pred):.4f}")
    print(f"Precision: {precision_score(y_true, y_pred):.4f}")
    print(f"Recall:    {recall_score(y_true, y_pred):.4f}")
    print(f"F1 Score:  {f1_score(y_true, y_pred):.4f}")
    if y_proba is not None:
        print(f"AUC:       {roc_auc_score(y_true, y_proba):.4f}")

from sklearn.metrics import ConfusionMatrixDisplay

def show_confusion_matrix(y_true, y_pred, model_name):
    disp = ConfusionMatrixDisplay.from_predictions(
        y_true, y_pred,
        display_labels=['No CHD', 'CHD'],
        cmap='Blues'
    )
    disp.ax_.set_title(f'Confusion Matrix: {model_name}')
    plt.grid(False)
    plt.show()

def plot_roc_curve(y_true, y_proba, model_name):
    fpr, tpr, thresholds = roc_curve(y_true, y_proba)
    auc_score = auc(fpr, tpr)

    plt.figure(figsize=(6, 5))
    plt.plot(fpr, tpr, label=f"AUC = {auc_score:.2f}", color='navy')
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(f"ROC Curve - {model_name}")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

"""# **1. Logistic Regression**"""

from sklearn.linear_model import LogisticRegression

lr_model = LogisticRegression(max_iter=1000,random_state=SEED)
lr_model.fit(Xf_train, yf_train)

# Predict
y_lr_pred = lr_model.predict(Xf_test)
y_lr_proba = lr_model.predict_proba(Xf_test)[:, 1]

# Evaluate
print("🔹 Logistic Regression Performance:")
evaluate_model(yf_test, y_lr_pred, y_lr_proba)

print("🔹 Classification Report — Logistic Regression")
print(classification_report(yf_test, y_lr_pred, target_names=['No CHD', 'CHD']))

# Confusion Matrix
show_confusion_matrix(yf_test, y_lr_pred, "Logistic Regression")

y_lr_proba = lr_model.predict_proba(Xf_test)[:, 1]
plot_roc_curve(yf_test, y_lr_proba, "Logistic Regression")

"""# **2. Decision Tree Classifier**"""

from sklearn.tree import DecisionTreeClassifier

dt_model = DecisionTreeClassifier(random_state=SEED)
dt_model.fit(Xf_train, yf_train)

# Predict
y_dt_pred = dt_model.predict(Xf_test)
y_dt_proba = dt_model.predict_proba(Xf_test)[:, 1]

# Evaluate
print("🔹 Decision Tree Performance:")
evaluate_model(yf_test, y_dt_pred, y_dt_proba)

print("🔹 Classification Report — Decision Tree")
print(classification_report(yf_test, y_dt_pred, target_names=['No CHD', 'CHD']))

show_confusion_matrix(yf_test, y_dt_pred, "Decision Tree")

y_dt_proba = dt_model.predict_proba(Xf_test)[:, 1]
plot_roc_curve(yf_test, y_dt_proba, "Decision Tree")

"""# **3. Random Forest Classifier**"""

from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(n_estimators=100, random_state=SEED)
rf_model.fit(Xf_train, yf_train)

y_rf_pred = rf_model.predict(Xf_test)
y_rf_proba = rf_model.predict_proba(Xf_test)[:, 1]

print("🔹 Random Forest Performance:")
evaluate_model(yf_test, y_rf_pred, y_rf_proba)

print("🔹 Classification Report — Random Forest")
print(classification_report(yf_test, y_rf_pred, target_names=['No CHD', 'CHD']))

show_confusion_matrix(yf_test, y_rf_pred, "Random Forest")

y_rf_proba = rf_model.predict_proba(Xf_test)[:, 1]
plot_roc_curve(yf_test, y_rf_proba, "Random Forest")

"""# **4. Support Vector Machine (SVM)**"""

from sklearn.svm import SVC

svm_model = SVC(probability=True, random_state=SEED)
svm_model.fit(Xf_train, yf_train)

y_svm_pred = svm_model.predict(Xf_test)
y_svm_proba = svm_model.predict_proba(Xf_test)[:, 1]

print("🔹 Support Vector Machine Performance:")
evaluate_model(yf_test, y_svm_pred, y_svm_proba)

print("🔹 Classification Report — SVM")
print(classification_report(yf_test, y_svm_pred, target_names=['No CHD', 'CHD']))

show_confusion_matrix(yf_test, y_svm_pred, "SVM")

y_svm_proba = svm_model.predict_proba(Xf_test)[:, 1]
plot_roc_curve(yf_test, y_svm_proba, "Support Vector Machine")

"""# **5. K-Nearest Neighbors (KNN)**"""

from sklearn.neighbors import KNeighborsClassifier

knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(Xf_train, yf_train)

y_knn_pred = knn_model.predict(Xf_test)
y_knn_proba = knn_model.predict_proba(Xf_test)[:, 1]

print("🔹 K-Nearest Neighbors Performance:")
evaluate_model(yf_test, y_knn_pred, y_knn_proba)

print("🔹 Classification Report — KNN")
print(classification_report(yf_test, y_knn_pred, target_names=['No CHD', 'CHD']))

show_confusion_matrix(yf_test, y_knn_pred, "KNN")

y_knn_proba = knn_model.predict_proba(Xf_test)[:, 1]
plot_roc_curve(yf_test, y_knn_proba, "K-Nearest Neighbors")

"""# **6. Gradient Boosting Classifier**"""

from sklearn.ensemble import GradientBoostingClassifier

gb_model = GradientBoostingClassifier(random_state=SEED)
gb_model.fit(Xf_train, yf_train)

y_gb_pred = gb_model.predict(Xf_test)
y_gb_proba = gb_model.predict_proba(Xf_test)[:, 1]

print("🔹 Gradient Boosting Performance:")
evaluate_model(yf_test, y_gb_pred, y_gb_proba)

print("🔹 Classification Report — Gradient Boosting")
print(classification_report(yf_test, y_gb_pred, target_names=['No CHD', 'CHD']))

show_confusion_matrix(yf_test, y_gb_pred, "Gradient Boosting")

y_gb_proba = gb_model.predict_proba(Xf_test)[:, 1]
plot_roc_curve(yf_test, y_gb_proba, "Gradient Boosting")

"""# **7. Naive Bayes**"""

from sklearn.naive_bayes import GaussianNB

nb_model = GaussianNB()
nb_model.fit(Xf_train, yf_train)

y_nb_pred = nb_model.predict(Xf_test)
y_nb_proba = nb_model.predict_proba(Xf_test)[:, 1]

print("🔹 Naive Bayes Performance:")
evaluate_model(yf_test, y_nb_pred, y_nb_proba)

print("🔹 Classification Report — Naive Bayes")
print(classification_report(yf_test, y_nb_pred, target_names=['No CHD', 'CHD']))

show_confusion_matrix(yf_test, y_nb_pred, "Naive Bayes")

y_nb_proba = nb_model.predict_proba(Xf_test)[:, 1]
plot_roc_curve(yf_test, y_nb_proba, "Naive Bayes")

"""# **Model Comparision**"""

models = {
    'Logistic Regression': LogisticRegression(max_iter=1000,random_state=SEED),
    'Decision Tree': DecisionTreeClassifier(random_state=SEED),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=SEED),
    'SVM': SVC(probability=True, random_state=SEED),
    'KNN': KNeighborsClassifier(n_neighbors=5),
    'Gradient Boosting': GradientBoostingClassifier(random_state=SEED),
    'Naive Bayes': GaussianNB()
}

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score
)

# Store all results
results = []

for name, model in models.items():
    model.fit(Xf_train, yf_train)
    y_pred = model.predict(Xf_test)
    y_proba = model.predict_proba(Xf_test)[:, 1] if hasattr(model, "predict_proba") else None

    accuracy = accuracy_score(yf_test, y_pred)
    precision = precision_score(yf_test, y_pred)
    recall = recall_score(yf_test, y_pred)
    f1 = f1_score(yf_test, y_pred)
    auc = roc_auc_score(yf_test, y_proba) if y_proba is not None else None

    results.append({
        'Model': name,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1,
        'AUC': auc
    })

#Create DataFrame of Results
df_results = pd.DataFrame(results)
df_results.set_index('Model', inplace=True)
df_results

df_results.plot(kind='bar', figsize=(12, 6), colormap='Set2')
plt.title('Model Performance Comparison - Framingham Dataset')
plt.ylabel('Score')
plt.ylim(0, 1.0)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""# **Cross-Validation for Each Model**"""

from sklearn.model_selection import cross_val_score

# Create a new results list
cv_results = []

for name, model in models.items():
    # Use 5-fold cross-validation on accuracy
    accuracy = cross_val_score(model, X_fhs_scaled, y_fhs, cv=5, scoring='accuracy').mean()
    precision = cross_val_score(model, X_fhs_scaled, y_fhs, cv=5, scoring='precision').mean()
    recall = cross_val_score(model, X_fhs_scaled, y_fhs, cv=5, scoring='recall').mean()
    f1 = cross_val_score(model, X_fhs_scaled, y_fhs, cv=5, scoring='f1').mean()
    auc = cross_val_score(model, X_fhs_scaled, y_fhs, cv=5, scoring='roc_auc').mean()

    cv_results.append({
        'Model': name,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1,
        'AUC': auc
    })

df_cv_results = pd.DataFrame(cv_results).set_index('Model')
df_cv_results

"""# **Rank Models by Interpretability**"""

interpretability = {
    'Logistic Regression': 'High',
    'Decision Tree': 'High',
    'Random Forest': 'Medium',
    'Gradient Boosting': 'Medium',
    'Naive Bayes': 'Low',
    'SVM': 'Low',
    'KNN': 'Low'
}

df_cv_results['Interpretability'] = df_cv_results.index.map(interpretability)
df_cv_results

df_cv_results.sort_values(by=['F1 Score', 'Interpretability'], ascending=[False, True])

"""# **Statistical Testing: Paired t-test Between Top Models**"""

from scipy.stats import ttest_rel
from sklearn.model_selection import cross_val_score

# Pick top 2 models
top_model_names = df_cv_results.sort_values(by='F1 Score', ascending=False).head(6).index.tolist()

# Cross-validated F1 scores
scores_1 = cross_val_score(models[top_model_names[2]], X_fhs_scaled, y_fhs, cv=5, scoring='f1')
scores_2 = cross_val_score(models[top_model_names[5]], X_fhs_scaled, y_fhs, cv=5, scoring='f1')

# Paired t-test
t_stat, p_val = ttest_rel(scores_1, scores_2)

print(f"Paired t-test between {top_model_names[2]} and {top_model_names[5]}:")
print(f"t-statistic = {t_stat:.4f}, p-value = {p_val:.4f}")
if p_val < 0.05:
    print("✅ The performance difference is statistically significant.")
else:
    print("⚠️ The difference is not statistically significant.")

from scipy.stats import ttest_rel
from sklearn.model_selection import cross_val_score

# Pick top 2 models
top_model_names = df_cv_results.sort_values(by='F1 Score', ascending=False).head(2).index.tolist()

# Cross-validated F1 scores
scores_1 = cross_val_score(models[top_model_names[0]], X_fhs_scaled, y_fhs, cv=5, scoring='f1')
scores_2 = cross_val_score(models[top_model_names[1]], X_fhs_scaled, y_fhs, cv=5, scoring='f1')

# Paired t-test
t_stat, p_val = ttest_rel(scores_1, scores_2)

print(f"Paired t-test between {top_model_names[0]} and {top_model_names[1]}:")
print(f"t-statistic = {t_stat:.4f}, p-value = {p_val:.4f}")
if p_val < 0.05:
    print("✅ The performance difference is statistically significant.")
else:
    print("⚠️ The difference is not statistically significant.")

"""# **Feature Importance from Random Forest**"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Use feature names from df_fhs (excluding the target)
feature_names = df_fhs.drop(columns=['TenYearCHD']).columns.tolist()

# Get importances from trained model
importances = rf_model.feature_importances_

# Create DataFrame and plot
feat_imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feat_imp_df = feat_imp_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feat_imp_df)
plt.title("Feature Importance - Random Forest (Framingham Dataset)")
plt.tight_layout()
plt.show()

"""# **Hyperparameter Tuning with GridSearchCV**"""

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}


grid_rf = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    scoring='f1',
    cv=5,
    n_jobs=-1,
    verbose=1
)

grid_rf.fit(Xf_train, yf_train)

# Best model
best_rf = grid_rf.best_estimator_

print("Best Parameters:", grid_rf.best_params_)

#Evaluate Tuned Model
y_best_rf_pred = best_rf.predict(Xf_test)
y_best_rf_proba = best_rf.predict_proba(Xf_test)[:, 1]

print(classification_report(yf_test, y_best_rf_pred))

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Predict probabilities for class 1 (MI)
y_best_rf_proba = best_rf.predict_proba(Xf_test)[:, 1]

fpr, tpr, thresholds = roc_curve(yf_test, y_best_rf_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, label=f"Tuned Random Forest (AUC = {roc_auc:.2f})", color='darkorange')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

show_confusion_matrix(yf_test, y_best_rf_pred, "Tuned Random Forest")

"""# **Pipeline B.1: Deep Learning on Raw ECG**

# **LSTM pipeline Using Raw ECG signals**
"""

!pip install wfdb
!pip install torch torchvision
!nvidia-smi

import zipfile
import os

from google.colab import drive
drive.mount('/content/drive')

# Path to ZIP file in Google Drive
zip_path = "/content/drive/My Drive/ptb-xl.zip"

# Extraction path inside Colab
extract_path = "/content/ptbxl_data"

# Create the extraction directory
os.makedirs(extract_path, exist_ok=True)

# Extract files
with zipfile.ZipFile(zip_path, "r") as zip_ref:
    zip_ref.extractall(extract_path)

print("Extraction complete! Files are saved in:", extract_path)

# Check metadata CSV exists or not
metadata_path = os.path.join(extract_path, "/content/ptbxl_data/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/ptbxl_database.csv") #ptbxl_database.csv
print("Metadata file found:", os.path.exists(metadata_path))

import pandas as pd
import ast

# Load metadata
df_ptbxl = pd.read_csv('/content/ptbxl_data/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/ptbxl_database.csv')

df_ptbxl.columns

df_ptbxl.shape

df_ptbxl.info()

df_ptbxl.describe()

# Check for missing values
df_ptbxl.isnull().sum()

import os
import ast
import wfdb
import torch
import numpy as np
import pandas as pd
import torch.nn as nn
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve

import random
import torch

SEED = 42
random.seed(SEED)
np.random.seed(SEED)

# Load metadata
df_ptbxl = pd.read_csv('/content/ptbxl_data/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/ptbxl_database.csv')
scp_df = pd.read_csv('/content/ptbxl_data/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/scp_statements.csv', index_col='Unnamed: 0')


# Get MI-related codes
mi_keys = scp_df[scp_df['diagnostic_class'] == 'MI'].index.tolist()

# Create binary label: MI = 1, non-MI = 0
def is_mi(scp_code_str):
    try:
        codes = ast.literal_eval(scp_code_str)
        return int(any(code in mi_keys for code in codes))
    except:
        return 0

df_ptbxl['label'] = df_ptbxl['scp_codes'].apply(is_mi)
print(df_ptbxl['label'].value_counts())

# Balance classes
df_mi = df_ptbxl[df_ptbxl['label'] == 1]
df_non_mi = df_ptbxl[df_ptbxl['label'] == 0].sample(n=len(df_mi), random_state=SEED)
df_balanced = pd.concat([df_mi, df_non_mi]).sample(frac=1, random_state=SEED).reset_index(drop=True)

# ⚡ Optional: Reduce for fast testing
df_balanced = df_balanced.sample(n=4000, random_state=SEED)

# Split
df_train, df_test = train_test_split(df_balanced, test_size=0.2, stratify=df_balanced['label'], random_state=42)

"""# **PyTorch Dataset for ECG Signals**"""

class PTBXLDataset(Dataset):
    def __init__(self, df, base_path, signal_length=750):
        self.df = df
        self.base_path = base_path
        self.signal_length = signal_length

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        label = torch.tensor(row['label'], dtype=torch.float32)
        file_path = os.path.join(self.base_path, row['filename_lr'])

        record = wfdb.rdrecord(file_path)
        signal = record.p_signal[:self.signal_length, :]
        epsilon = 1e-8
        signal = (signal - signal.mean(axis=0)) / (signal.std(axis=0) + epsilon)
       # signal = (signal - signal.mean(axis=0)) / signal.std(axis=0)

        if signal.shape[0] < self.signal_length:
            pad_len = self.signal_length - signal.shape[0]
            pad = np.zeros((pad_len, 12))
            signal = np.vstack((signal, pad))

        return torch.tensor(signal, dtype=torch.float32), label

"""# **Create DataLoaders**"""

base_path = '/content/ptbxl_data/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3'


train_dataset = PTBXLDataset(df_train, base_path)
test_dataset = PTBXLDataset(df_test, base_path)

# train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, pin_memory=True, num_workers=2)
# test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, pin_memory=True, num_workers=2)

from torch.utils.data import DataLoader
import numpy as np

# 💡 SEED should already be set earlier in your code
SEED = 42

# Reproducibility for DataLoader workers
def seed_worker(worker_id):
    np.random.seed(SEED + worker_id)

g = torch.Generator()
g.manual_seed(SEED)

# ✅ Add these reproducible DataLoaders
train_loader = DataLoader(
    train_dataset,
    batch_size=128,
    shuffle=True,
    pin_memory=True,
    num_workers=2,
    worker_init_fn=seed_worker,
    generator=g
)

test_loader = DataLoader(
    test_dataset,
    batch_size=128,
    shuffle=False,
    pin_memory=True,
    num_workers=2,
    worker_init_fn=seed_worker,
    generator=g
)

len(train_loader.dataset)

len(test_loader.dataset)

import seaborn as sns
import pandas as pd

# Assuming labels for train_dataset are available or created earlier
labels_b = [label for _, label in train_dataset]

sns.countplot(x=labels_b)
plt.title("Class Balance - Pipeline B (LSTM Raw ECG)")
plt.xlabel("Label (0 = Non-MI, 1 = MI)")
plt.ylabel("Count")
plt.show()

sample_signal, sample_label = train_dataset[0]

# Convert tensor to NumPy for plotting
import matplotlib.pyplot as plt

sample_signal = sample_signal.numpy().squeeze()  # remove channel if present
plt.plot(sample_signal)
plt.title(f"Sample ECG Signal (Label: {sample_label})")
plt.xlabel("Time Steps")
plt.ylabel("Amplitude")
plt.grid(True)
plt.show()

# ECG Signal Plot
signal, label = train_dataset[0]
signal = signal.numpy()  # shape = [750, 12]

plt.figure(figsize=(12, 8))
for i in range(12):
    plt.plot(signal[:, i] + i * 5, label=f'Lead {i+1}')  # offset each lead
plt.title(f"Sample ECG (Label: {int(label)})")
plt.xlabel("Time Steps")
plt.ylabel("Amplitude (offset by channel)")
plt.legend(loc="upper right")
plt.grid(True)
plt.show()

# Combine amplitudes across all leads from a sample
flat_signal = signal.flatten()
sns.histplot(flat_signal, bins=50, kde=True, color="purple")
plt.title("Amplitude Distribution - Sample ECG")
plt.xlabel("Amplitude")
plt.ylabel("Frequency")
plt.grid(True)
plt.show()

"""# **LSTM Model**"""

class TunedLSTM(nn.Module):
    def __init__(self, input_size=12, hidden_size=64, num_layers=2):
        super(TunedLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size=input_size,
                            hidden_size=hidden_size,
                            num_layers=num_layers,
                            batch_first=True,
                            bidirectional=True,
                            dropout=0.3)
        self.dropout = nn.Dropout(0.4)
        self.fc = nn.Linear(hidden_size * 2, 1)  # *2 for bidirectional

    def forward(self, x):
        _, (hn, _) = self.lstm(x)
        hn_cat = torch.cat((hn[-2], hn[-1]), dim=1)  # concat directions
        out = self.dropout(hn_cat)
        return torch.sigmoid(self.fc(out)).squeeze()

"""# **CNN Model**"""

class ECG_CNN(nn.Module):
    def __init__(self, input_length=750, num_classes=1):
        super(ECG_CNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv1d(12, 32, kernel_size=5, padding=2),
            nn.ReLU(),
            nn.MaxPool1d(2),
            nn.Conv1d(32, 64, kernel_size=5, padding=2),
            nn.ReLU(),
            nn.MaxPool1d(2)
        )
        self.flattened_size = (input_length // 4) * 64
        self.fc = nn.Linear(self.flattened_size, num_classes)

    def forward(self, x):
        x = x.permute(0, 2, 1)  # Change to [batch, channels, time]
        x = self.conv(x)
        x = x.view(x.size(0), -1)
        return torch.sigmoid(self.fc(x)).squeeze()

"""# **Train the Model**"""

def train_model(model, loader, criterion, optimizer, epochs=100):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for x_batch, y_batch in loader:
            x_batch, y_batch = x_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            outputs = model(x_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch+1}: Loss = {total_loss/len(loader):.4f}")

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, classification_report, confusion_matrix, roc_curve,
    ConfusionMatrixDisplay
)
import matplotlib.pyplot as plt
import numpy as np

def evaluate_model(model, loader, model_name="Model", return_predictions=False):
    model.eval()
    y_true, y_pred, y_prob = [], [], []

    with torch.no_grad():
        for x_batch, y_batch in loader:
            x_batch = x_batch.to(device)
            probs = model(x_batch).cpu().numpy()
            preds = (probs >= 0.5).astype(int)

            y_true.extend(y_batch.numpy())
            y_pred.extend(preds)
            y_prob.extend(probs)

    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    y_prob = np.array(y_prob)

    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred, zero_division=0)
    rec = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    auc = roc_auc_score(y_true, y_prob)

    # ✅ Classification Report
    print(f"\n📊 {model_name} Evaluation:")
    print(f"Accuracy : {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1 Score: {f1:.4f} | AUC: {auc:.4f}")
    print("\nClassification Report:\n", classification_report(y_true, y_pred, target_names=["Non-MI", "MI"]))

    # ✅ ROC Curve
    fpr, tpr, _ = roc_curve(y_true, y_prob)
    plt.figure(figsize=(6, 4))
    plt.plot(fpr, tpr, label=f"AUC = {auc:.2f}", color='blue')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.title(f"ROC Curve - {model_name}")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.legend()
    plt.grid(True)
    plt.show()

    # ✅ Confusion Matrix
    cm = confusion_matrix(y_true, y_pred)
    ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Non-MI", "MI"]).plot(cmap="Blues")
    plt.title(f"Confusion Matrix - {model_name}")
    plt.grid(False)
    plt.show()

    # ✅ Return for fusion if requested
    if return_predictions:
        return y_true, y_prob

    # Otherwise return metrics only
    return {
        "Model": model_name,
        "Accuracy": round(acc, 4),
        "Precision": round(prec, 4),
        "Recall": round(rec, 4),
        "F1 Score": round(f1, 4),
        "AUC": round(auc, 4)
    }

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
results_pipeline_b = []

# 🔁 LSTM
model = TunedLSTM().to(device)
criterion = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)
train_model(model, train_loader, criterion, optimizer, epochs=100)
results_pipeline_b.append(evaluate_model(model, test_loader, "LSTM"))

# ➕ 1D CNN
model = ECG_CNN(input_length=750).to(device)
criterion = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)
train_model(model, train_loader, criterion, optimizer, epochs=100)
results_pipeline_b.append(evaluate_model(model, test_loader, "1D CNN"))

y_lstm_true, y_lstm_proba = evaluate_model(model, test_loader, model_name="LSTM", return_predictions=True)

results_df = pd.DataFrame(results_pipeline_b)
print(results_df)

import seaborn as sns
sns.set(style="whitegrid")
results_df.set_index("Model").plot(kind="bar", figsize=(10,6))
plt.title("Pipeline B: LSTM vs 1D CNN Performance")
plt.ylabel("Score")
plt.grid(True)
plt.show()

"""# **Pipeline B.2: ML on Engineered ECG Features**

# **Extract features from ECG waveforms**
"""

!pip install wfdb
!pip install neurokit2
!pip install scikit-learn
!pip install imbalanced-learn

import zipfile
import os
import wfdb
import neurokit2 as nk
import numpy as np
import pandas as pd
import scipy.signal
import multiprocessing
from joblib import Parallel, delayed
from sklearn.impute import KNNImputer
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from joblib import Parallel, delayed

from google.colab import drive
drive.mount('/content/drive')

# Path to ZIP file in Google Drive
zip_path = "/content/drive/My Drive/ptb-xl.zip"

# Extraction path inside Colab
extract_path = "/content/ptbxl_data"

# Create the extraction directory
os.makedirs(extract_path, exist_ok=True)

# Extract files
with zipfile.ZipFile(zip_path, "r") as zip_ref:
    zip_ref.extractall(extract_path)

print("Extraction complete! Files are saved in:", extract_path)

# List of extracted files
for root, dirs, files in os.walk(extract_path):
    print(root, "contains", len(files), "files")

# Check metadata CSV exists or not
metadata_path = os.path.join(extract_path, "/content/ptbxl_data/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/ptbxl_database.csv") #ptbxl_database.csv
print("Metadata file found:", os.path.exists(metadata_path))

ecg_dir = os.path.join(extract_path, "/content/ptbxl_data/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/records500/00000")
print("Files in directory:", os.listdir(ecg_dir)[:5])  # Show first 5 files

"""# **ECG Feature Engineering**"""

!pip install wfdb neurokit2 scikit-learn imbalanced-learn

import pandas as pd
import os
import ast
import numpy as np
import neurokit2 as nk
import wfdb
import warnings
from joblib import Parallel, delayed

# Load metadata
df_ptbxl = pd.read_csv('/content/ptbxl_data/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/ptbxl_database.csv')
scp_df = pd.read_csv('/content/ptbxl_data/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/scp_statements.csv', index_col='Unnamed: 0')

# Get MI-related SCP codes
mi_keys = scp_df[scp_df['diagnostic_class'] == 'MI'].index.tolist()

# Label function
def is_mi(scp_code_str):
    try:
        codes = ast.literal_eval(scp_code_str)
        return int(any(code in mi_keys for code in codes))
    except:
        return 0

df_ptbxl['label'] = df_ptbxl['scp_codes'].apply(is_mi)
print("✅ MI label distribution:\n", df_ptbxl['label'].value_counts())

base_path = "/content/ptbxl_data/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/records500"
record_list = []

for root, dirs, files in os.walk(base_path):
    for file in files:
        if file.endswith(".dat"):
            full_path = os.path.join(root, file[:-4])
            record_list.append(full_path)

print("✅ Total ECG records found:", len(record_list))

sampling_rate = 500
num_workers = 2
batch_size = 200

def compute_hrv_metrics(rpeaks_idx):
    if len(rpeaks_idx) < 2:
        return None
    rr_intervals = np.diff(rpeaks_idx) * (1000 / sampling_rate)
    mean_nn = np.mean(rr_intervals)
    sdnn = np.std(rr_intervals, ddof=1)
    rmssd = np.sqrt(np.mean(np.square(np.diff(rr_intervals))))
    return mean_nn, sdnn, rmssd

# --- ECG Feature Extractor ---
def extract_ecg_features(record_path):
    try:
        if not os.path.exists(record_path + ".dat"):
            return None
        signals, _ = wfdb.rdsamp(record_path)
        signals = signals.astype(np.float32)
        if signals.shape[0] == 0 or np.all(signals == 0):
            return None

        _, rpeaks = nk.ecg_peaks(signals[:, 0], sampling_rate=sampling_rate)
        rpeaks_idx = rpeaks["ECG_R_Peaks"]
        if len(rpeaks_idx) < 2:
            return None
        hrv_vals = compute_hrv_metrics(rpeaks_idx)
        if hrv_vals is None:
            return None
        mean_nn, sdnn, rmssd = hrv_vals
        qrs_duration = mean_nn

        with warnings.catch_warnings():
            warnings.simplefilter("ignore", category=RuntimeWarning)
            ecg_signals, _ = nk.ecg_process(signals[:, 0], sampling_rate=sampling_rate)

        ecg_signals = ecg_signals.fillna(0).astype(np.float32)

        pr_interval = (
            abs(np.nanmean(ecg_signals["ECG_Q_Peaks"] - ecg_signals["ECG_P_Peaks"])) / sampling_rate
            if "ECG_Q_Peaks" in ecg_signals and "ECG_P_Peaks" in ecg_signals else 0
        )
        qt_interval = (
            abs(np.nanmean(ecg_signals["ECG_T_Peaks"] - ecg_signals["ECG_Q_Peaks"])) / sampling_rate
            if "ECG_T_Peaks" in ecg_signals and "ECG_Q_Peaks" in ecg_signals else 0
        )
        # st_elevation = (
        #     np.ptp(ecg_signals["ECG_ST_Segment"]) if "ECG_ST_Segment" in ecg_signals else 0
        # )
        heart_rate = (
            np.nanmean(ecg_signals["ECG_Rate"]) if "ECG_Rate" in ecg_signals else 0
        )
        ####
        # record_name = os.path.basename(record_path)
        # Add Lead-specific features (optional per lead if you process multiple leads)
        qrs_amplitude = np.ptp(signals[:, 0])  # Peak-to-peak amplitude in Lead I
        signal_energy = np.sum(np.square(signals[:, 0]))  # Energy of the signal
        signal_entropy = -np.sum(np.nan_to_num(ecg_signals["ECG_Clean"] * np.log(ecg_signals["ECG_Clean"] + 1e-8)))

        return {
            # "record_name": record_name,
            "HRV_MeanNN": mean_nn,
            "HRV_SDNN": sdnn,
            "HRV_RMSSD": rmssd,
            "QRS_Duration": qrs_duration,
            "PR_interval": pr_interval,
            "QT_interval": qt_interval,
            # "ST_elevation": st_elevation,
            "Heart_Rate": heart_rate,
            # Then add to your return dictionary:
            "QRS_Amplitude": qrs_amplitude,
            "Signal_Energy": signal_energy,
            "Signal_Entropy": signal_entropy,

        }

    except:
        return None


def process_in_batches(records, batch_size=200):
    all_features = []
    for i in range(0, len(records), batch_size):
        batch = records[i:i+batch_size]
        print(f"Processing batch {i}-{i+len(batch)}...")
        batch_features = Parallel(n_jobs=num_workers)(
            delayed(extract_ecg_features)(rec) for rec in batch
        )
        all_features.extend(filter(None, batch_features))
    return all_features

# Limit samples for faster testing
# record_list = record_list[:400]
ecg_features = process_in_batches(record_list)
ecg_features_df = pd.DataFrame(ecg_features)

ecg_features_df.head(5)

# print("Extracted ECG Features Overview:")
ecg_features_df.info()

ecg_features_df.describe()

ecg_features_df.isnull().sum()

"""# **Data Visualization**"""

#Class Distribution
import seaborn as sns
import matplotlib.pyplot as plt

# sns.countplot(x=y)
df_ptbxl['label'].value_counts().plot.bar()
plt.title("Distribution of MI vs Non-MI Cases")
plt.xlabel("Label (0 = Non-MI, 1 = MI)")
plt.ylabel("Count")
plt.grid(True)
plt.show()

# 📈 Correlation Heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(ecg_features_df.corr(), annot=True, fmt=".2f")
plt.title("Correlation Between ECG Features")
plt.show()

# ECG record path to visualize (after record_list is built)
sample_record = record_list[0]

# Load ECG signal
signals, _ = wfdb.rdsamp(sample_record)
signal = signals[:, 0]

# Detect R-peaks
_, rpeaks = nk.ecg_peaks(signal, sampling_rate=500)

# Plot ECG with R-peaks
plt.figure(figsize=(15, 4))
plt.plot(signal, label="ECG Signal")
plt.plot(rpeaks["ECG_R_Peaks"], signal[rpeaks["ECG_R_Peaks"]], "ro", label="R-peaks")
plt.title("ECG Signal with Detected R-peaks")
plt.xlabel("Samples")
plt.ylabel("Amplitude")
plt.legend()
plt.show()

import matplotlib.pyplot as plt

# Set larger figure and adjust layout
ax = ecg_features_df.hist(
    bins=20,
    figsize=(14, 10),
    color="skyblue",
    edgecolor="black",
    grid=False
)

# Rotate x-axis labels for readability
for axes in ax.flatten():
    axes.set_xlabel(axes.get_title(), fontsize=10)
    axes.set_title("")  # Remove duplicate titles
    axes.tick_params(axis='x', rotation=45)

plt.suptitle("Histogram of ECG Signal Features", fontsize=16, y=1.02)
plt.tight_layout()
plt.show()

# Prepare matching keys
df_ptbxl['record_id'] = df_ptbxl['filename_hr'].apply(lambda x: os.path.basename(x).replace(".dat", ""))
filenames = [os.path.basename(path).replace('.dat', '') for path in record_list[:len(ecg_features_df)]]

# Match records
df_labels_filtered = df_ptbxl[df_ptbxl['record_id'].isin(filenames)]
df_labels_filtered = df_labels_filtered.set_index('record_id').loc[filenames]

# Final feature matrix and target
X = ecg_features_df.iloc[:len(filenames)].values
y = df_labels_filtered['label'].values

# print("✅ X shape:", X.shape)
# print("✅ y shape:", y.shape)

import numpy as np

# Combine X and y into a DataFrame for safe filtering
df_Xy = pd.DataFrame(X)
df_Xy['label'] = y

# Drop rows with any NaNs
df_Xy = df_Xy.dropna()

# Separate again
X = df_Xy.drop(columns='label').values
y = df_Xy['label'].values

# print("✅ After dropna:", X.shape, y.shape)

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, classification_report, confusion_matrix, roc_curve
)
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

"""# **One By One**"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

SEED = 42
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=SEED)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# **Train Model**"""

results = []  # This will collect metrics for comparison

def evaluate_model(name, model, X_train, X_test, y_train, y_test, return_proba=False, results=None):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Use probability or decision function
    if hasattr(model, "predict_proba"):
        y_proba = model.predict_proba(X_test)[:, 1]
    else:
        y_proba = model.decision_function(X_test)

    # Metrics
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, zero_division=0)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_proba)

    # ✅ Append to results if provided
    if results is not None:
        results.append({
            "Model": name,
            "Accuracy": acc,
            "Precision": prec,
            "Recall": rec,
            "F1 Score": f1,
            "AUC": auc
        })

    # 📊 Print metrics
    print(f"\n📌 {name} Evaluation:")
    print(f"Accuracy : {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1 Score: {f1:.4f} | AUC: {auc:.4f}")
    print("\nClassification Report:\n", classification_report(y_test, y_pred, target_names=["Non-Disease", "Disease"]))

    # 🧩 Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Non-Disease", "Disease"])
    disp.plot(cmap='Blues')
    plt.title(f"{name} - Confusion Matrix")
    plt.grid(False)
    plt.show()

    # 📈 ROC Curve
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    plt.figure(figsize=(6, 4))
    plt.plot(fpr, tpr, label=f"AUC = {auc:.2f}", color='blue')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.title(f"{name} - ROC Curve")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.legend()
    plt.grid(True)
    plt.show()

    # ✅ Return probabilities for late fusion (optional)
    if return_proba:
        return y_test, y_proba
    else:
        return None, None

"""# **LogisticRegression**"""

from sklearn.linear_model import LogisticRegression

lr_model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=SEED)
_, _ = evaluate_model("Logistic Regression", lr_model, X_train_scaled, X_test_scaled, y_train, y_test)

"""# **Decision Tree**"""

dt_model = DecisionTreeClassifier(class_weight='balanced', max_depth=5, random_state=SEED)
_, _ = evaluate_model("Decision Tree", dt_model, X_train_scaled, X_test_scaled, y_train, y_test, results=results)

"""# **Random Forest**"""

rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=SEED)
y_ptbxl_b2, y_ptbxl_proba = evaluate_model("Random Forest", rf_model, X_train_scaled, X_test_scaled, y_train, y_test, return_proba=True, results=results)

"""# **SVM**"""

svm_model = SVC(kernel='linear', class_weight='balanced', probability=True, random_state=SEED)
_, _ = evaluate_model("SVM (Linear)", svm_model, X_train_scaled, X_test_scaled, y_train, y_test, results=results)

"""# **KNN**"""

knn_model = KNeighborsClassifier(n_neighbors=5)
_, _ = evaluate_model("KNN", knn_model, X_train_scaled, X_test_scaled, y_train, y_test, results=results)

import pandas as pd

# Create and sort the DataFrame
results_df = pd.DataFrame(results)
results_df = results_df.sort_values(by="AUC", ascending=False).reset_index(drop=True)

# Display in styled format
print("📊 Model Performance Comparison:")
display(results_df.style.background_gradient(cmap='Blues').format(precision=3))

"""# **Late Fusion**

# **Implement Late Fusion**
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, classification_report, confusion_matrix, roc_curve
)

"""# **Check Shape of Probability Arrays**

y_rf_proba: From RandomForestClassifier.predict_proba(Xf_test)[:, 1]

y_prob: From evaluate_model(model, test_loader) return (LSTM probs)

yf_test: Ground-truth labels used for Xf_test in Framingham
"""

n = min(len(y_best_rf_proba), len(y_lstm_proba), len(y_ptbxl_proba), len(yf_test))

# Trim all to same length
proba_rf = y_best_rf_proba[:n]
proba_lstm = y_lstm_proba[:n]
proba_ptbxl = y_ptbxl_proba[:n]
y_true_fused = yf_test[:n]

# ✅ Step 2: Average probabilities (equal weighting)
fused_proba = (proba_rf + proba_lstm + proba_ptbxl) / 3
fused_pred = (fused_proba >= 0.5).astype(int)

# ✅ Step 3: Evaluate fusion performance
print("Late Fusion Evaluation")
print("Accuracy :", accuracy_score(y_true_fused, fused_pred))
print("Precision:", precision_score(y_true_fused, fused_pred, zero_division=0))
print("Recall   :", recall_score(y_true_fused, fused_pred))
print("F1 Score :", f1_score(y_true_fused, fused_pred))
print("AUC Score:", roc_auc_score(y_true_fused, fused_proba))
print("\nClassification Report:\n", classification_report(y_true_fused, fused_pred))

# ✅ Step 4: Confusion Matrix
cm = confusion_matrix(y_true_fused, fused_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["No Disease", "Disease"], yticklabels=["No Disease", "Disease"])
plt.title("Confusion Matrix - Late Fusion (All Pipelines)")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.grid(False)
plt.show()

# ✅ Step 5: ROC Curve
fpr, tpr, _ = roc_curve(y_true_fused, fused_proba)
plt.plot(fpr, tpr, label=f"AUC = {roc_auc_score(y_true_fused, fused_proba):.2f}", color='darkorange')
plt.plot([0, 1], [0, 1], 'k--')
plt.title("ROC Curve - Late Fusion (All Pipelines)")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.grid(True)
plt.show()